--- gamess/rungms	2014-09-11 11:27:09.956415197 -0700
+++ patch-files/rungms	2014-09-11 11:26:59.772966855 -0700
@@ -59,10 +59,17 @@
 #       both Sun Grid Engine (SGE), and Portable Batch System (PBS).
 #       See also a very old LoadLeveler "ll-gms" for some IBM systems.
 #
-set TARGET=sockets
-set SCR=/scr/$USER
-set USERSCR=~$USER/scr
-set GMSPATH=/u1/mike/gamess
+set TARGET=mpi
+#set SCR=/scr/$USER
+#set USERSCR=~$USER/scr
+#set GMSPATH=/u1/mike/gamess
+set USERSCR=$PWD
+if ( ! $?GAMESS_SCR )  then
+  set SCR=$PWD
+else
+  set SCR=$GAMESS_SCR
+endif
+set GMSPATH=ROLL_PATHGAMESS
 #
 set JOB=$1      # name of the input file xxx.inp, give only the xxx part
 set VERNO=$2    # revision number of the executable created by 'lked' step
@@ -92,9 +99,9 @@
    uniq $TMPDIR/machines
 endif
 if ($SCHED == PBS) then
-   set SCR=/scratch/$PBS_JOBID
-   echo "PBS has assigned the following compute nodes to this run:"
-   uniq $PBS_NODEFILE
+#   set SCR=/scratch/$PBS_JOBID
+    echo "PBS has assigned the following compute nodes to this run:"
+    uniq $PBS_NODEFILE
 endif
 #
 echo "Available scratch disk space (Kbyte units) at beginning of the job is"
@@ -516,7 +523,7 @@
    #      we'll pass in a "processers per node" value, that is,
    #      all nodes are presumed to have equal numbers of cores.
    #
-   set PPN=$4
+   set PPN=$PBS_NUM_PPN
    #
    #      Allow for compute process and data servers (one pair per core)
    #      note that NCPUS = #cores, and NPROCS = #MPI processes
@@ -528,7 +535,7 @@
    #         specify your MPI library's top level path just below,
    #         this will have directories like include/lib/bin below it.
    #
-   set DDI_MPI_CHOICE=impi
+   set DDI_MPI_CHOICE=ROLL_MPITYPE
    #
    #        ISU's various clusters have various iMPI paths
    #          the examples are our dynamo/chemphys2011/exalted/bolt clusters
@@ -817,17 +824,17 @@
          set echo
          setenv I_MPI_HYDRA_ENV all
          setenv I_MPI_PERHOST $PPN2
+         mpirun -machinefile $PROCFILE -np $NPROCS \
+            $GMSPATH/gamess.$VERNO.x < /dev/null
          unset echo
       endif
       if ($DDI_MPI_CHOICE == mvapich2) then
          set echo
          setenv HYDRA_ENV all
+         mpirun_rsh -hostfile $PROCFILE -np $NPROCS \
+            $GMSPATH/gamess.$VERNO.x < /dev/null
          unset echo
       endif
-      set echo
-      mpiexec.hydra -f $PROCFILE -n $NPROCS \
-            $GMSPATH/gamess.$VERNO.x < /dev/null
-      unset echo
       breaksw
    #
    case default:
